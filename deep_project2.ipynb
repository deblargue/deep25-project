{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjvLYn6uWW19"
   },
   "source": [
    "# Deep Learning Project (DD2424)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- INITIAL SETUP --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSXlKp53WWUL"
   },
   "source": [
    "### 0.1 Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "download_data = True",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if download_data:\n",
    "    !wget https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz\n",
    "    !wget https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz\n",
    "\n",
    "    !mkdir oxford-iiit-pet/\n",
    "    !tar -xf images.tar.gz -C oxford-iiit-pet/\n",
    "    !tar -xf annotations.tar.gz -C oxford-iiit-pet/\n",
    "    !rm images.tar.gz\n",
    "    !rm annotations.tar.gz\n",
    "\n",
    "else:\n",
    "    print(\"Note: Assuming that data is manually downloaded!\")\n",
    "    # --> Alternative is to manually download datasets here: https://www.robots.ox.ac.uk/~vgg/data/pets/\n",
    "    #  Note: must have the folder \"oxford-iiit-pet\" in same directory as code/notebook"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Define Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.transforms import transforms"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DL5Naf6TWkCg"
   },
   "source": [
    "### 0.3 Example plot"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "id": "e_fI4Jr0WoFF",
    "outputId": "3c4a5117-b60b-40aa-851c-2e10798907f6"
   },
   "source": [
    "from torchvision.io import read_image\n",
    "\n",
    "# -- PLOT EXAMPLE IMAGE AND MASK --\n",
    "name = 'samoyed_189'\n",
    "img = read_image(f'oxford-iiit-pet/images/{name}.jpg')\n",
    "mask = read_image(f'oxford-iiit-pet/annotations/trimaps/{name}.png')\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(mask.permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- DATA PREPROCESSING --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define datasets and dataloaders and subset splits (train, validation, test):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rRnA2xofXkog"
   },
   "source": [
    "# --------- Transform pipeline ---------\n",
    "\n",
    "def transform_pipeline(in_data):\n",
    "    # Define an ordered collection of functions/transformations to apply to all images\n",
    "    pipeline = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "    ])\n",
    "    return pipeline(in_data)\n",
    "\n",
    "def load_dataset(target_type=\"binary-category\", ):\n",
    "    ttransform = target_transform if target_type==\"category\" else None\n",
    "    # Load in all the data within file 'trainval.txt'\n",
    "    return OxfordIIITPet(root='.', download=False, transform=transform_pipeline, target_transform=ttransform, target_types=[target_type] )\n",
    "    #return dataset\n",
    "\n",
    "def target_transform(target):\n",
    "\n",
    "    hot = torch.zeros((37))\n",
    "\n",
    "    hot[target-1] = 1\n",
    "    #cat_classes = {6, 34, 12, 27, 33, 7, 24, 21, 10, 28, 1, 8}  # set lookup is faster\n",
    "    #return 0 if target in cat_classes else 1\n",
    "\n",
    "    return hot   # if we use [\"binary-category\"]\n",
    "\n",
    "def create_split_idxs(n_tot, f_train=0.8, f_valid=0.1, f_test=0.1):\n",
    "\n",
    "    if f_train + f_valid + f_test > 1:\n",
    "        print(\"ERROR: FACTORS TOO LARGE TO CREATE SPLITS, TRY AGAIN\")\n",
    "\n",
    "    # Range from index 0 to n==len(dataset)\n",
    "    idxes = np.arange(n_tot)\n",
    "    np.random.seed(42)   # random seed to ensure repeatability\n",
    "    np.random.shuffle(idxes)\n",
    "\n",
    "    # split limits:\n",
    "    s1 = int(n_tot*f_train)\n",
    "    s2 = int(n_tot*(f_train+f_valid))\n",
    "\n",
    "    # Create index lists\n",
    "    train_idx = idxes[:s1]\n",
    "    val_idx = idxes[s1:s2]\n",
    "    test_idx = idxes[s2:]\n",
    "\n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "\n",
    "def get_dataloaders(n_batch, f_train=0.8, f_valid=0.1, f_test=0.1, target_type=\"binary-category\"):\n",
    "    # Load in all data\n",
    "    dataset = load_dataset(target_type=target_type)\n",
    "\n",
    "    # Test it\n",
    "    image, label = dataset[0]\n",
    "    print(f\"Image shape: {image.shape}\")\n",
    "\n",
    "    # Create shuffled index splitting for each subset\n",
    "    train_idx, val_idx, test_idx = create_split_idxs(len(dataset), f_train, f_valid, f_test)\n",
    "\n",
    "    # Split using imported Subset class\n",
    "    dataset_train = Subset(dataset, train_idx)\n",
    "    dataset_test = Subset(dataset, test_idx)\n",
    "    dataset_val = Subset(dataset, val_idx)\n",
    "\n",
    "    # Create data loader containing datasets\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=n_batch, shuffle=True, num_workers=2)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=n_batch, shuffle=False, num_workers=2)\n",
    "    dataloader_val = DataLoader(dataset_val, batch_size=n_batch, shuffle=True, num_workers=2)\n",
    "\n",
    "    return dataloader_train, dataloader_test, dataloader_val\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Plot random example images to show pre-processing (scaling and cropping)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OalCpdn0YhE-"
   },
   "source": [
    "dataset = load_dataset(\"category\")\n",
    "\n",
    "#print(len(dataset[0]))\n",
    "\n",
    "# plot example image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(121)\n",
    "\n",
    "dog = 2600\n",
    "plt.imshow(dataset[dog][0].permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.title(f\"label={dataset[dog][1]}\")\n",
    "plt.subplot(122)\n",
    "\n",
    "cat = 300\n",
    "plt.imshow(dataset[cat][0].permute(1, 2, 0))\n",
    "plt.title(f\"label={dataset[cat][1]}\")\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (maybe todo) 1.3 Define zero-mean normalization across all data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def zero_mean(dataloader):\n",
    "    # TODO: FIXME IF WE WANT TO DO ZERO MEAN NORMALIZATION\n",
    "\n",
    "    #print(next(iter(dataloader_test))[0].shape)  # torch.Size([64, 3, 224, 224])\n",
    "    #img = next(iter(dataloader_test))[0][0]\n",
    "    #print(img.max())\n",
    "    #print(img.min())\n",
    "    #print(max(next(iter(dataloader_test))))\n",
    "\n",
    "    img, _ = next(iter(dataloader))\n",
    "    img = img[0]\n",
    "    print(f'max: {torch.max(img)}')\n",
    "    print(f'min: {torch.min(img)}')\n",
    "    print(f'mean: {torch.mean(img)}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuIlzB-QdMDv"
   },
   "source": [
    "## -- NETWORK SETUP --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Network Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ldGRVsNwdN24"
   },
   "source": [
    "from torchvision.models import resnet18\n",
    "from torchvision.models import resnet34\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models import resnet101\n",
    "from torchvision.models import wide_resnet50_2\n",
    "#from torchvision.models import resnet152\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define model and training functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O2EtTN-ydWld"
   },
   "source": [
    "def create_model_and_optim_binary(choice=\"resnet34\", lr=0.001, wd=0.0005):\n",
    "\n",
    "    # Load in pretrained model of our choice:\n",
    "    if choice == \"resnet18\":\n",
    "        model = resnet18(weights='DEFAULT')   # Using best (pretrained?) weights with \"DEFAULT\"\n",
    "    elif choice == \"resnet34\":\n",
    "        model = resnet34(weights='DEFAULT')   # Using best (pretrained?) weights with \"DEFAULT\"\n",
    "    elif choice == \"resnet50\":\n",
    "        model = resnet50(weights='DEFAULT')   # Using best (pretrained?) weights with \"DEFAULT\"\n",
    "    elif choice == \"resnet101\":\n",
    "        model = resnet101(weights='DEFAULT')   # Using best (pretrained?) weights with \"DEFAULT\"\n",
    "    else:\n",
    "        print(\"model choice does not match\")\n",
    "        exit()\n",
    "\n",
    "    # As instructed, only replace final layer with same in but one out (cat/dog)\n",
    "    in_features = model.fc.in_features   # size of input of final layer --> 512\n",
    "    #print(model.fc.out_features)   # size of output into final layer --> 1000\n",
    "\n",
    "    # Create a new Linear layer to replace last fc layer with:\n",
    "    model.fc = torch.nn.Linear(in_features, 1)  # for binary classification instead of outputting 1000\n",
    "\n",
    "    # Freeze all layers before final layer which we will replace\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True   # update these layer weights (only last)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.fc.parameters(), lr=lr, weight_decay=wd)\n",
    "    # TODO: maybe look into NAG as alternative\n",
    "\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def train_binary(model, dataloader, optimizer, epochs=1):\n",
    "    losses = []\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.train()  # tells model we will train now maybe\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    # trains in batches\n",
    "    for e in range(epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # for each batch:\n",
    "        for i, (img, binary_category) in enumerate(dataloader):\n",
    "\n",
    "            #print(img.shape)  # torch.Size([64, 3, 224, 224])\n",
    "\n",
    "            img = img.to(\"cuda\")\n",
    "            binary_category = binary_category.to(\"cuda\")\n",
    "\n",
    "            # Forward pass of network\n",
    "            output = model(img)\n",
    "\n",
    "            # Compute loss\n",
    "            # TODO: check if we want to use another loss function\n",
    "            loss = criterion(output, binary_category.view(-1, 1).float())\n",
    "\n",
    "            # Compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.step()  # update weights with grads\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "\n",
    "            losses.append(batch_loss)\n",
    "            running_loss += batch_loss\n",
    "\n",
    "            print(f\"\\r Epoch {e+1}/{epochs} : Batch {i+1}/{len(dataloader)} : Loss {batch_loss}\", end=\"\")\n",
    "\n",
    "            optimizer.zero_grad()  # reset grads\n",
    "\n",
    "    return losses\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- NETWORK TRAINING --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Initialize data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataloader_train, dataloader_test, dataloader_val = get_dataloaders(n_batch=32, f_train=0.7, f_test=0.1, f_valid=0.2, target_type=\"binary-category\")\n",
    "\n",
    "print(\"done loading data\")\n",
    "print(\"number of batches:\", len(dataloader_train))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VpkwlY7qfjT3",
    "outputId": "59617089-34a6-42ee-b031-a1ffc294190f"
   },
   "source": [
    "print(f\"cuda == {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_binary, optimizer_binary = create_model_and_optim_binary(choice=\"resnet34\", lr=0.001, wd=0.0005)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train binary network"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#print(torch.cuda.get_device_name(0))  # Tesla P4\n",
    "\n",
    "print(\"Start training\")\n",
    "losses = train_binary(model_binary, dataloader_train, optimizer_binary, epochs=1)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Plot losses and compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def smooth(y, box_pts=10):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth\n",
    "\n",
    "def plot_losses(losses):\n",
    "    #losses_cpu = [loss.cpu().detach().numpy() for loss in losses]\n",
    "    losses_cpu = [loss for loss in losses]\n",
    "    #plt.plot(smooth(losses_cpu))\n",
    "    plt.plot(losses_cpu)\n",
    "    plt.xlabel(\"batch nr\")\n",
    "    plt.ylabel(\"training loss\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot\n",
    "plot_losses(losses)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Binary Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: FIXME\n",
    "def compute_accuracy_binary(model, dataloader_test):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    # no grad so it's faster (--> Julia edit: 'no_grad()' ensures that we can calculate with tensors without affecting it's gradients in backprop)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for i, (img, binary_category) in enumerate(dataloader_test):\n",
    "            img = img.to(\"cuda\")\n",
    "            binary_category = binary_category.to(\"cuda\").view(-1, 1).float()   # torch.Size([32, 1])\n",
    "\n",
    "            output = model(img)                                     # torch.Size([32, 1])\n",
    "            prediction = torch.round(torch.sigmoid(output))         # torch.Size([32, 1])\n",
    "\n",
    "            total += binary_category.size(0)\n",
    "            correct += (prediction == binary_category).sum().item()\n",
    "\n",
    "            print(f'\\r({i+1}/{len(dataloader_test)}) accuracy: {100 * correct / total} %', end=\"\")\n",
    "        print(f'\\nTest accuracy: {100 * correct / total} %')\n",
    "\n",
    "\n",
    "def compute_confusion_matrix():\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# NOTE/WARNING:\n",
    "compute_accuracy_binary(model_binary, dataloader_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- MULTICLASS --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Modify network for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_model_and_optim_multi(choice=\"resnet34\", lr=0.001, wd=0.0005, n_freeze=1, separate_scheduler=False):\n",
    "\n",
    "    # Load in pretrained model of our choice:\n",
    "    if choice == \"resnet18\":\n",
    "        model = resnet18(weights='DEFAULT')   # Using best (pretrained?) weights with \"DEFAULT\"\n",
    "    elif choice == \"resnet34\":\n",
    "        model = resnet34(weights='DEFAULT')   # Using best (pretrained?) weights with \"DEFAULT\"\n",
    "    elif choice == \"resnet50\":\n",
    "        model = resnet50(weights='DEFAULT')   # Using best (pretrained?) weights with \"DEFAULT\"\n",
    "    elif choice == \"resnet101\":\n",
    "        model = resnet101(weights='DEFAULT')   # Using best (pretrained?) weights with \"DEFAULT\"\n",
    "    else:\n",
    "        print(\"model choice does not match\")\n",
    "        exit()\n",
    "\n",
    "    # Create a new Linear layer to replace last fc layer with:\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, 37)  # for binary classification instead of outputting 1000\n",
    "\n",
    "    modules = [model.fc, model.layer4, model.layer3, model.layer2, model.layer1]\n",
    "    layers_to_unfreeze = modules[:n_freeze]\n",
    "\n",
    "    # Freeze all layers before final layer which we will replace\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze the ones we want\n",
    "    for layer in layers_to_unfreeze:\n",
    "        #print(\"freeeeze\")\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    optimizers = []\n",
    "    schedulers = []\n",
    "\n",
    "    if separate_scheduler:\n",
    "        for i, layer in enumerate(layers_to_unfreeze):\n",
    "            optimizers.append(torch.optim.Adam(\n",
    "                filter(lambda p: p.requires_grad, layer.parameters()),\n",
    "                lr=lr*0.1**i, # exponential decay of lr for each layer\n",
    "                weight_decay=wd\n",
    "            ))\n",
    "            schedulers.append(torch.optim.lr_scheduler.StepLR(optimizers[i], step_size=3, gamma=0.1))\n",
    "    else:\n",
    "        optimizers.append(torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=lr,\n",
    "            weight_decay=wd\n",
    "        ))\n",
    "\n",
    "        schedulers.append(torch.optim.lr_scheduler.StepLR(optimizers[0], step_size=3, gamma=0.1))\n",
    "\n",
    "    return model, optimizers, schedulers\n",
    "\n",
    "def train_multi(model, dataloader, optimizers, epochs=1, schedulers=[], weight=None):\n",
    "    losses = []\n",
    "    model = model.to(device)\n",
    "    model.train()  # tells model we will train now maybe\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "    # trains in batches\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        # for each batch:\n",
    "        for i, (img, category) in enumerate(dataloader):\n",
    "            img = img.to(\"cuda\")\n",
    "            category = category.squeeze().float().to(\"cuda\")\n",
    "            output = model(img) # Forward pass of network\n",
    "\n",
    "            loss = criterion(output, category) # TODO: check if we want to use another loss function\n",
    "            loss.backward() # Compute gradients\n",
    "            for optimizer in optimizers:\n",
    "                optimizer.step()# Backprop # update weights with grads\n",
    "            batch_loss = loss.item()\n",
    "            losses.append(batch_loss)\n",
    "            running_loss += batch_loss\n",
    "            print(f\"\\r Epoch {e+1}/{epochs} : Batch {i+1}/{len(dataloader)} : Loss {batch_loss}\", end=\"\")\n",
    "            optimizer.zero_grad()  # reset grads\n",
    "        for scheduler in schedulers:\n",
    "            scheduler.step()\n",
    "\n",
    "    return losses"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load in multiclass data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataloader_train, dataloader_test, dataloader_val = get_dataloaders(n_batch=32, f_train=0.7, f_test=0.1, f_valid=0.2, target_type=\"category\")\n",
    "\n",
    "print(\"done loading multiclass data\")\n",
    "print(\"number of batches:\", len(dataloader_train))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### 2.3 Initialize model, train, and plot losses\n",
    "print(f\"cuda == {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model1, optimizers1, schedulers1 = create_model_and_optim_multi(choice=\"resnet50\", lr=0.0005, wd=0.0005, n_freeze=3)\n",
    "#model2, optimizer2, scheduler2 = create_model_and_optim_multi(choice=\"resnet34\", lr=0.001, wd=0.0005, n_freeze=2)\n",
    "#model3, optimizer3, scheduler3 = create_model_and_optim_multi(choice=\"resnet34\", lr=0.001, wd=0.0005, n_freeze=3)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Train multiclass model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Start training\")\n",
    "losses = train_multi(model1, dataloader_train, optimizers1, epochs=10, schedulers=schedulers1)\n",
    "#losses2 = train_multi(model2, dataloader_train, optimizer2, epochs=1)\n",
    "#losses3 = train_multi(model3, dataloader_train, optimizer3, epochs=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Multiclass metric calculations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: FIXME\n",
    "def compute_accuracy_multi(model, dataloader_test):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    model = model.to(device)\n",
    "\n",
    "    # no grad so it's faster (--> Julia edit: I think 'no_grad()' ensures that we can calculate with tensors without affecting its gradients in backprop)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (img, category) in enumerate(dataloader_test):\n",
    "            img = img.to(\"cuda\")\n",
    "            category = torch.argmax(category, axis=1).to(\"cuda\")  # torch.Size([32, 37]) ---> torch.Size([32])\n",
    "\n",
    "            output = model(img)  # torch.Size([32, 37])\n",
    "            # from output choose class/index with the highest value (probability?)\n",
    "            prediction = torch.argmax(output, axis=1)   # torch.Size([32])\n",
    "\n",
    "            total += category.size(0)\n",
    "            correct += (prediction == category).sum().item()\n",
    "\n",
    "    return correct / total\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_predictions(model, dataloader_test):\n",
    "    model = model.to(device)\n",
    "    predictions = np.zeros((len(dataloader_test.dataset)))\n",
    "\n",
    "    # no grad so it's faster (--> Julia edit: I think 'no_grad()' ensures that we can calculate with tensors without affecting its gradients in backprop)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (img, category) in enumerate(dataloader_test):\n",
    "            img = img.to(\"cuda\")\n",
    "            category = torch.argmax(category, axis=1).to(\"cuda\")  # torch.Size([32, 37]) ---> torch.Size([32])\n",
    "\n",
    "            output = model(img)  # torch.Size([32, 37])\n",
    "            # from output choose class/index with the highest value (probability?)\n",
    "            prediction = torch.argmax(output, axis=1)   # torch.Size([32])\n",
    "            predictions[i*32:(i+1)*32] = prediction.cpu().numpy()\n",
    "\n",
    "    return predictions"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision and recall, then F1 macro score.\n",
    "\n",
    "$ precision = \\frac{TP}{TP + FP} $\n",
    "\n",
    "$ recall = \\frac{TP}{TP + FN} $\n",
    "\n",
    "$ F_1 = \\frac{2}{recall^{-1} + precision^{-1}} = \\frac{2TP}{2TP + FP + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot distribution of true classes and predicted classes:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compute_confusion_matrix(preds, y_test):\n",
    "    TP = np.zeros((37))\n",
    "    FP = np.zeros((37))\n",
    "    TN = np.zeros((37))\n",
    "    FN = np.zeros((37))\n",
    "\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] == y_test[i]:\n",
    "            TP[int(preds[i])] += 1\n",
    "        else:\n",
    "            FP[int(preds[i])] += 1\n",
    "            FN[int(y_test[i])] += 1\n",
    "            TN[int(y_test[i])] += 1\n",
    "\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "# find f1 score for each class\n",
    "def f1_score(TP, FP, TN, FN):\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1.mean()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def bin_conf_matrix_from_class(TP, FP, TN, FN):\n",
    "    TP_BIN = np.sum(TP[:12])\n",
    "    FP_BIN = np.sum(FP[:12])\n",
    "    TN_BIN = np.sum(TP[12:])\n",
    "    FN_BIN = np.sum(FP[12:])\n",
    "    return TP_BIN, FP_BIN, TN_BIN, FN_BIN\n",
    "\n",
    "def f1_score_binary(model, dataloader_test):\n",
    "    TP, FP, TN, FN = compute_confusion_matrix(get_predictions(model, dataloader_test), y_test)\n",
    "    TP_BIN, FP_BIN, TN_BIN, FN_BIN = bin_conf_matrix_from_class(TP, FP, TN, FN)\n",
    "    return f1_score(TP_BIN, FP_BIN, TN_BIN, FN_BIN)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- EXPERIMENTS --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Strategy 1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train_multi_simul(num_layers, dataloader_train):\n",
    "    print(f\"\\n=== Training with {num_layers} layers unfreezed ===\")\n",
    "    model, optimizers, schedulers = create_model_and_optim_multi(\n",
    "            choice=\"resnet50\",\n",
    "            lr=0.0005, \n",
    "            wd=0.0005, \n",
    "            n_freeze=num_layers,\n",
    "            separate_scheduler=True # lwering lr exponentially for each layer\n",
    "        )\n",
    "    losses = train_multi(model, dataloader_train, optimizers, epochs=5, schedulers=schedulers)\n",
    "    \n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_layers_to_unfreeze = [1,2,3,4]\n",
    "strategy1_models = []\n",
    "\n",
    "for num_layers in num_layers_to_unfreeze:\n",
    "    model = train_multi_simul(num_layers, dataloader_train)\n",
    "    strategy1_models.append(model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for i in range(len(strategy1_models)):\n",
    "    print(f\"\\n=== Evaluating model with {num_layers_to_unfreeze[i]} layers unfreezed ===\")\n",
    "    acc = compute_accuracy_multi(strategy1_models[i], dataloader_test)\n",
    "    print(f\"Test accuracy: {100 * acc:.2f}%\")\n",
    "\n",
    "    # save all models\n",
    "    torch.save({\n",
    "        'model_state_dict': strategy1_models[i].state_dict(),\n",
    "        'accuracy': acc,\n",
    "    }, f'strategy1_model_{num_layers_to_unfreeze[i]}_layers.pth')\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Strategy 2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train_multi_gradual(num_layers, dataloader_train):\n",
    "    # Start with just the final layer\n",
    "    model, optimizers, schedulers = create_model_and_optim_multi(\n",
    "            choice=\"resnet50\",\n",
    "            lr=0.0005, \n",
    "            wd=0.0005, \n",
    "            n_freeze=1, \n",
    "            separate_scheduler=True\n",
    "        )\n",
    "\n",
    "    for stage in range(1, num_layers+1):  # Gradually unfreeze more layers\n",
    "        print(f\"Fine-tuning last {stage} layers\")\n",
    "        \n",
    "        # Unfreeze additional layer\n",
    "        if stage > 1:\n",
    "            modules = [model.fc, model.layer4, model.layer3, model.layer2]\n",
    "            layer_to_unfreeze = modules[stage-1]\n",
    "            \n",
    "            for param in layer_to_unfreeze.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "            # add optimizer for the new layer\n",
    "            optimizers.append(torch.optim.Adam(\n",
    "                layer_to_unfreeze.parameters(),\n",
    "                lr=0.0005*0.1**(stage-1), # exponential decay of lr for each layer\n",
    "                weight_decay=0.0005\n",
    "            ))\n",
    "            # add scheduler for the new layer\n",
    "            schedulers.append(torch.optim.lr_scheduler.StepLR(optimizers[-1], step_size=3, gamma=0.1))\n",
    "        \n",
    "        # train for a few epochs\n",
    "        train_multi(model, dataloader_train, optimizers, epochs=2, schedulers=schedulers)\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataloader_train, dataloader_test, dataloader_val = get_dataloaders(n_batch=32, f_train=0.7, f_test=0.1, f_valid=0.2, target_type=\"category\")\n",
    "strategy2_models = []\n",
    "\n",
    "for num_layers in num_layers_to_unfreeze:\n",
    "    print(f\"\\n=== Training with {num_layers} layers unfreezed ===\")\n",
    "    model = train_multi_gradual(num_layers, dataloader_train)\n",
    "    strategy2_models.append(model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for i in range(len(strategy2_models)):\n",
    "    print(f\"\\n=== Evaluating model with {num_layers_to_unfreeze[i]} layers unfreezed ===\")\n",
    "    acc = compute_accuracy_multi(strategy2_models[i], dataloader_test)\n",
    "    print(f\"Test accuracy: {100 * acc:.2f}%\")\n",
    "\n",
    "    # save all models\n",
    "    torch.save({\n",
    "        'model_state_dict': strategy2_models[i].state_dict(),\n",
    "        'accuracy': acc,\n",
    "    }, f'strategy2_model_{num_layers_to_unfreeze[i]}_layers.pth')\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# transform pipeline for data augmentation\n",
    "def transform_pipeline_augmented(in_data):\n",
    "    \"\"\"Transform pipeline with data augmentation\"\"\"\n",
    "    pipeline = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "    ])\n",
    "    return pipeline(in_data)\n",
    "\n",
    "# create augmented dataset\n",
    "def load_dataset_augmented(target_type=\"category\"):\n",
    "    ttransform = target_transform if target_type==\"category\" else None\n",
    "    return OxfordIIITPet(root='.', download=False, transform=transform_pipeline_augmented,\n",
    "                        target_transform=ttransform, target_types=[target_type])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the best model of strategy 1 and 2 and train with data augmentation. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train with augmentation\n",
    "dataset_aug = load_dataset_augmented(target_type=\"category\")\n",
    "train_idx, val_idx, test_idx = create_split_idxs(len(dataset_aug), 0.7, 0.1, 0.2)\n",
    "\n",
    "dataloader_train_aug = DataLoader(Subset(dataset_aug, train_idx), batch_size=32, shuffle=True, num_workers=2)\n",
    "dataloader_test_aug = DataLoader(Subset(dataset_aug, test_idx), batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "model_aug_s1 = train_multi_simul(2, dataloader_train_aug)\n",
    "model_aug_s2 = train_multi_gradual(2, dataloader_train_aug)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(compute_accuracy_multi(model_aug_s1, dataloader_test))\n",
    "print(compute_accuracy_multi(model_aug_s2, dataloader_test))\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model_aug_s1.state_dict(),\n",
    "    'accuracy': compute_accuracy_multi(model_aug_s1, dataloader_test),\n",
    "}, f'models/strategy1_model_aug_2_layers.pth')\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model_aug_s2.state_dict(),\n",
    "    'accuracy': compute_accuracy_multi(model_aug_s2, dataloader_test),\n",
    "}, f'models/strategy2_model_aug_2_layers.pth')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Imbalanced Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_imbalanced_dataset(dataloader_train, imbalance_ratio=0.2):\n",
    "    \"\"\"Create an imbalanced dataset by reducing cat breed samples\"\"\"\n",
    "    cat_classes = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}  # First 12 classes are cats\n",
    "    \n",
    "    imbalanced_data = []\n",
    "    imbalanced_labels = []\n",
    "    \n",
    "    # Process each sample\n",
    "    for img, label in dataloader_train.dataset:\n",
    "        class_idx = torch.argmax(label).item()\n",
    "        \n",
    "        if class_idx in cat_classes:\n",
    "            # Keep only 20% of cat breeds\n",
    "            if np.random.random() < imbalance_ratio:\n",
    "                imbalanced_data.append(img)\n",
    "                imbalanced_labels.append(label)\n",
    "        else:\n",
    "            # Keep all dog breeds\n",
    "            imbalanced_data.append(img)\n",
    "            imbalanced_labels.append(label)\n",
    "    \n",
    "    # Create TensorDataset NOTE: might not work\n",
    "    imbalanced_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.stack(imbalanced_data),\n",
    "        torch.stack(imbalanced_labels),\n",
    "    )\n",
    "    \n",
    "    return DataLoader(imbalanced_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create imbalanced dataset\n",
    "imbalanced_loader = create_imbalanced_dataset(dataloader_train, imbalance_ratio=0.05)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class_counts = torch.zeros(37)\n",
    "for _, labels in imbalanced_loader:\n",
    "    for label in labels:\n",
    "        class_idx = torch.argmax(label).item()\n",
    "        class_counts[class_idx] += 1\n",
    "\n",
    "class_weights = 1 / (class_counts + 1e-6)\n",
    "class_weights = class_weights / class_weights.mean()  # Normalize"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "y_test = np.zeros((len(dataloader_test.dataset)))\n",
    "for i, (img, category) in enumerate(dataloader_test):\n",
    "    y_test[i*32:(i+1)*32] = torch.argmax(category, axis=1).cpu().numpy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unweighted cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_simul_imb = train_multi_simul(2, imbalanced_loader)\n",
    "model_gradual_imb = train_multi_gradual(2, imbalanced_loader)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"f1 score imb classes: \", f1_score_binary(model_simul_imb, dataloader_test))\n",
    "print(\"f1 score standard classes: \", f1_score_binary(model_aug_s1, dataloader_test))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted cross-entropy training\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "m_imb_w, opt_w, sch_w = create_model_and_optim_multi(\n",
    "        choice=\"resnet50\", \n",
    "        lr=0.0005, \n",
    "        wd=0.0005, \n",
    "        n_freeze=2\n",
    "    )\n",
    "l_imb_w = train_multi(\n",
    "        m_imb_w, \n",
    "        imbalanced_loader, \n",
    "        opt_w, \n",
    "        epochs=5, \n",
    "        schedulers=sch_w, \n",
    "        weight=class_weights.to(device)\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "f1 = f1_score_binary(m_imb_w, dataloader_test)\n",
    "print(\"f1 score imb classes: \", f1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## -- EXTENSION 1: Implementing ViT --"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.0 Imports??"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if True:\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch\n",
    "    from tqdm import tqdm\n",
    "    import torchvision\n",
    "\n",
    "    from torchvision.io import read_image\n",
    "    from torchvision.transforms import ToTensor\n",
    "    from torchvision.ops.boxes import masks_to_boxes\n",
    "\n",
    "    from torchvision.datasets import OxfordIIITPet\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torch.utils.data import Subset\n",
    "    from torchvision.transforms import transforms\n",
    "\n",
    "# ---- ABOVE IS DEFINED IN FIRST PART, BUT FOR NOW ONLY RUN EXTENSION CELLS---\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
    "from evaluate import load\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "breed_dict = {\n",
    "    1: \"Abyssinian\",\n",
    "    2: \"American Bulldog\",\n",
    "    3: \"American Pit Bull Terrier\",\n",
    "    4: \"Basset Hound\",\n",
    "    5: \"Beagle\",\n",
    "    6: \"Bengal\",\n",
    "    7: \"Birman\",\n",
    "    8: \"Bombay\",\n",
    "    9: \"Boxer\",\n",
    "    10: \"British Shorthair\",\n",
    "    11: \"Chihuahua\",\n",
    "    12: \"Egyptian Mau\",\n",
    "    13: \"English Cocker Spaniel\",\n",
    "    14: \"English Setter\",\n",
    "    15: \"German Shorthaired\",\n",
    "    16: \"Great Pyrenees\",\n",
    "    17: \"Havanese\",\n",
    "    18: \"Japanese Chin\",\n",
    "    19: \"Keeshond\",\n",
    "    20: \"Leonberger\",\n",
    "    21: \"Main Coon\",\n",
    "    22: \"Miniature Pinscher\",\n",
    "    23: \"Newfoundland\",\n",
    "    24: \"Persian\",\n",
    "    25: \"Pomeranian\",\n",
    "    26: \"Pug\",\n",
    "    27: \"Ragdoll\",\n",
    "    28: \"Russian Blue\",\n",
    "    29: \"Saint Bernard\",\n",
    "    30: \"Samoyed\",\n",
    "    31: \"Scottish Terrier\",\n",
    "    32: \"Shiba Inu\",\n",
    "    33: \"Siamese\",\n",
    "    34: \"Sphynx\",\n",
    "    35: \"Staffordshire Bull Terrier\",\n",
    "    36: \"Wheaten Terrier\",\n",
    "    37: \"Yorkshire Terrier\"\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (Only do once / first time) Convert directory structure to match for Dataset thing"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# FOLLOWING THIS: https://medium.com/@diego.machado/fine-tuning-vit-for-image-classification-with-hugging-face-48c4be31e367\n",
    "\n",
    "## Creating ðŸ¤— Dataset structure\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def restructure_dir_for_dataset():\n",
    "    #  ----1. Split Data\n",
    "\n",
    "    # Get file names and breed names\n",
    "    list_path = \"oxford-iiit-pet/annotations/list.txt\"\n",
    "    labels = pd.read_csv(list_path, sep=\" \", header=None, names=[\"file_name\", \"class_nr\", \"species_code\", \"species_nr\"])\n",
    "    print(labels)\n",
    "    print(labels.file_name[0])\n",
    "\n",
    "    X = labels.file_name      # id name of file\n",
    "    y = labels.class_nr       # breed name  --> CHANGED TO CLASS NUMBER TODO ??? KEEP NOTE OF\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13, shuffle = True, stratify = y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=13, shuffle = True, stratify = y_train)\n",
    "    train_data = pd.concat([X_train, y_train], axis = 1).reset_index(drop = True)\n",
    "    val_data = pd.concat([X_val, y_val], axis = 1).reset_index(drop = True)\n",
    "    test_data = pd.concat([X_test, y_test], axis = 1).reset_index(drop = True)\n",
    "\n",
    "\n",
    "    # CREATE AND PREPARE DIRECTORY STUCTURE\n",
    "    if not os.path.exists(\"PetClassification\"):\n",
    "        # NOTE ONLY DO ONCE:\n",
    "        # 2. make Dataset Dir and splits\n",
    "        DatasetDir = Path(\"PetClassification\")\n",
    "        os.mkdir(DatasetDir)\n",
    "        os.mkdir(DatasetDir/\"train\")\n",
    "        os.mkdir(DatasetDir/\"validation\")\n",
    "        os.mkdir(DatasetDir/\"test\")\n",
    "\n",
    "        # 3. make classes (breeds) dir\n",
    "        for class_nr in tqdm(train_data.class_nr.unique()):\n",
    "          os.mkdir(DatasetDir/\"train\"/f\"{class_nr}\")\n",
    "          os.mkdir(DatasetDir/\"validation\"/ f\"{class_nr}\")\n",
    "          os.mkdir(DatasetDir/\"test\"/f\"{class_nr}\")\n",
    "\n",
    "        # 4. moving images to split folder and breed\n",
    "        IMG_PATH = \"oxford-iiit-pet/images\"\n",
    "        def make_split_folder(split_df, split):\n",
    "          # iterate over dataset\n",
    "          for idx, row in tqdm(split_df.iterrows(), total = len(split_df), desc = f\"Making {split} folder\"):\n",
    "            img_name, class_nr = row\n",
    "            # copy files\n",
    "            shutil.copyfile(f\"{os.path.join(IMG_PATH, img_name)}.jpg\", f\"{DatasetDir/split/str(class_nr)/img_name}.jpg\")\n",
    "\n",
    "        make_split_folder(train_data, \"train\")\n",
    "        make_split_folder(val_data, \"validation\")\n",
    "        make_split_folder(test_data, \"test\")\n",
    "\n",
    "        print(\"DONE MOVING FILES\")\n",
    "    else:\n",
    "        print(\"Warning: Path already exists!\")\n",
    "\n",
    "# NOTE: ONLY DO THIS ONCE\n",
    "#restructure_dir_for_dataset()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.1  Load Hugging Face's image processor (ViT processor)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "To make sure we apply the correct transformations, we will use a ViTImageProcessor initialized with a configuration that was saved along with the pretrained model we plan to use. In our case, we'll be using the google/vit-base-patch16-224-in21k model, so let's load its image processor from the Hugging Face Hub.\n",
    "\"\"\"\n",
    "\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'    # TODO: LOOK INTO AND MAYBE PICK ANOTHER MODEL\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained(model_name_or_path)   # Load Hugging Face's image processor\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (MAYBE NOT NEEDED ANYMORE) 1.2 Define dataset functions and image transforms"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# NOTE FOR LATER: ALL THAT IS CHANGED IS THE TRANSFORM PIPELINE.\n",
    "\"\"\"\n",
    "# TODO: MAYBE, COMBINE WITH VERSION AT START OF FILE\n",
    "\n",
    "# TODO FIX MAYBE:\n",
    "transform_for_vit = transforms.Compose([\n",
    "        #transforms.Resize((224, 224)),     # NOTE: WE DONT CENTER CROP\n",
    "        #transforms.CenterCrop(224),\n",
    "        #transforms.ToTensor(),\n",
    "        transforms.PILToTensor(),   # from old version\n",
    "        transforms.ConvertImageDtype(torch.float),  # from old version\n",
    "        #transforms.Normalize(mean=processor.image_mean, std=processor.image_std)   # messes with processor???\n",
    "        ])\n",
    "\n",
    "#pipeline = transforms.Compose([\n",
    "#    transforms.Resize(224),\n",
    "#    transforms.CenterCrop(224),\n",
    "#    transforms.PILToTensor(),\n",
    "#    transforms.ConvertImageDtype(torch.float),\n",
    "#])\n",
    "\n",
    "def transform_pipeline_vit(in_data):  # Changed fro ViT\n",
    "    # Define transforms that match what ViT expects\n",
    "    return transform_for_vit(in_data)\n",
    "\n",
    "def load_dataset_vit(target_type=\"category\"):\n",
    "    ttransform = target_transform_vit if target_type==\"category\" else None\n",
    "    # Load in all the data within file 'trainval.txt'\n",
    "    return OxfordIIITPet(root='.', download=False, transform=transform_pipeline_vit, target_transform=ttransform, target_types=[target_type] )\n",
    "\n",
    "def target_transform_vit(target):   # 1-37\n",
    "    #print(target) # TODO: WARNING: NOTE: arg 'target' is 0-indexed // Julia 15 Maj\n",
    "    hot = torch.zeros((37))\n",
    "    hot[target] = 1   # NOTE: CHANGED FROM -->  # hot[target-1] = 1   # NOTE: PREVISOU VERSION SHIFTS EVERYTHING BACK ON STEP IN LIST\n",
    "    return hot   # if we use [\"binary-category\"]\n",
    "\"\"\"\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"# NOTE: LOADS IN FUUULLL DATASET\n",
    "dataset = load_dataset_vit(\"category\")\n",
    "\n",
    "def plot_ex():\n",
    "    # plot example image\n",
    "    ni = 2600\n",
    "    img0, label0 = dataset[ni]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(img0.permute(1, 2, 0))\n",
    "    plt.title(f\"label={label0}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plot_ex()\n",
    "\"\"\"\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.3 Load in dataset according to DataSet class used in tutorial"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Load in dataset\n",
    "ds = load_dataset(\"PetClassification\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Plot example from dataset\n",
    "ex = ds['train'][6200]\n",
    "plt.imshow(ex['image'])\n",
    "ex_nr = labels.int2str(ex['label'])\n",
    "plt.title(breed_dict[int(ex_nr)])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.4 Define processor and process dataset"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "To process an image, simply pass it to the image 'processor''s call function. This will return a dict containing pixel values, which is the numeric representation to be passed to the model. You get a NumPy array by default, but if you add the return_tensors='pt' argument, you'll get back torch tensors instead.\n",
    "\"\"\"\n",
    "\n",
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = processor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "    # Don't forget to include the labels!\n",
    "    inputs['labels'] = example_batch['label']\n",
    "    return inputs\n",
    "\n",
    "\n",
    "prepared_ds = ds.with_transform(transform)\n",
    "#print(prepared_ds['train'][0:2])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training and Evaluation ViT"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define Collate Function"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Batches are coming in as lists of dicts, so you can just unpack + stack those into batch tensors.\n",
    "Since the collate_fn will return a batch dict, you can **unpack the inputs to the model later\n",
    "\"\"\"\n",
    "\n",
    "# Define a collate function??? --> seems to put together batches of processed data  (adding dimension in first position??)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])  # stack???\n",
    "    }\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define Evaluation metric"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "The accuracy metric from evaluate can easily be used to compare the predictions with the labels. Below, you can see how to use it within a compute_metrics function that will be used by the Trainer.\n",
    "\"\"\"\n",
    "\n",
    "metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define label-to-class translations (unsure if needed)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: FIX AND LOOK INTO NOW THAT WE CHANGED TO DATASET\n",
    "def onehot_to_number(onehot_label):\n",
    "    return torch.argmax(onehot_label).item() + 1   # NOTE: Class numbers starts index at 1\n",
    "\n",
    "def number_to_name(num):\n",
    "    return breed_dict[num]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Some TODO's:\n",
    "* (DONE) Convert and use Dataset class according to tutorial for ViT\n",
    "\n",
    "* (DONE) Create 'prepared_ds' equivalent for our data and solution\n",
    "\n",
    "* (kind of DONE) Look into other models which we can load in to try (currently using from tutorial: 'google/vit-base-patch16-224-in21k')\n",
    "\n",
    "* (DONE) Split data into training, validation, testing\n",
    "\n",
    "* IMPORTANT: Change things to utilize torch and device='cuda', to ensure we USE the GPU and not the CPU\n",
    "* Try running everything on the GPU:\n",
    "    * note: Use all data\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initialize Pretrained model (ViTForImageClassification)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Let's load the pretrained model. We'll add num_labels on init so the model creates a classification head with the right number of units. We'll also include the id2label and label2id mappings to have human-readable labels in the Hub widget (if you choose to push_to_hub).\n",
    "\"\"\"\n",
    "\n",
    "labels = ds[\"train\"].features[\"label\"].names\n",
    "\n",
    "#idx2label = {str(idx): label for idx, label in enumerate(labels)}\n",
    "#label2idx = {label: str(idx) for idx, label in enumerate(labels)}\n",
    "# Get random int to test\n",
    "#print(f\"Breed: {idx2label[ds['train'][4000]['label']]}\")\n",
    "#plt.imshow(ds['train'][4000]['image'])\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Set up training arguments"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Almost ready to train! The last thing needed before that is to set up the training configuration by defining TrainingArguments.\n",
    "\n",
    "Most of these are pretty self-explanatory, but one that is quite important here is remove_unused_columns=False. This one will drop any features not used by the model's call function. By default it's True because usually it's ideal to drop unused feature columns, making it easier to unpack inputs into the model's call function. But, in our case, we need the unused features ('image' in particular) in order to create 'pixel_values'.\n",
    "\n",
    "What I'm trying to say is that you'll have a bad time if you forget to set remove_unused_columns=False.\n",
    "\"\"\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"/vit-base-oxford-pet\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=4,\n",
    "  fp16=True,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='none', #'tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Now, all instances can be passed to Trainer and we are ready to start training!\n",
    "\"\"\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds[\"train\"], # TODO: Create prepared_ds (see tutorial for context and figure out appropriate solution) https://huggingface.co/blog/fine-tune-vit\n",
    "    eval_dataset=prepared_ds[\"validation\"],\n",
    "    tokenizer=processor,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# NOTE: Remaining below has not been run or tested"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TRAIN ViT"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### EVALUATE ViT"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "metrics = trainer.evaluate(prepared_ds['validation'])\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "name": "deep_project2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deep-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
